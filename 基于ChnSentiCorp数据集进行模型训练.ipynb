{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7c2857-b581-4f9e-9190-22f042a1574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert中文情感模型预训练过程\n",
    "import torch#导入 PyTorch，一个流行的深度学习框架，用于构建和训练神经网络。\n",
    "from datasets import load_dataset#从 datasets 库中引入 load_dataset 方法，这个库可以方便地加载各种预定义的数据集\n",
    "from transformers import AdamW#初始化优化器和损失函数\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf01352-3a7d-4491-9fbe-6c498feb79c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb836020ad843f38aa4b021f95d2bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Adconna\\envs\\mypytorch1\\lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 设置设备，检查是否有 GPU 可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):#定义一个名为 Dataset 的类，继承自 torch.utils.data.Dataset，这是 PyTorch 中的一个基础类，用于创建自定义数据集。\n",
    "    def __init__(self, split):#初始化函数，在创建类的实例时调用。\n",
    "        #使用 datasets 库的 load_dataset 方法加载名为 ChnSentiCorp 的中文情感分类数据集\n",
    "        self.dataset = load_dataset('parquet',data_files={split:f'./ChnSentiCorp/{split}.parquet'})\n",
    "        self.split = split\n",
    "    def __len__(self):#定义 __len__ 方法，返回数据集的样本数。\n",
    "        return len(self.dataset[self.split])\n",
    "    def __getitem__(self, i):#定义 __getitem__ 方法，用于通过索引 i 获取单个样本。\n",
    "        text = self.dataset[self.split][i]['text']\n",
    "        label = self.dataset[self.split][i]['label']\n",
    "\n",
    "        return text, label#返回文本和标签，作为数据集的单个样本。\n",
    "\n",
    "\n",
    "dataset = Dataset('train')#创建 Dataset 类的实例，指定 split='train'，加载训练集数据。\n",
    "token = BertTokenizer.from_pretrained('./bert-base-chinese')\n",
    "\n",
    "#定义collate_fn对模型数据进行批处理\n",
    "def collate_fn(data):\n",
    "    # 从 data 中提取出每个样本的文本（sents）和标签（labels）\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    \n",
    "    #编码\n",
    "    #使用 token.batch_encode_plus 来批量编码文本。这个函数来自于 Hugging Face 的 transformers 库。\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,#batch_text_or_text_pairs=sents：要编码的文本列表。\n",
    "                                   truncation=True,#如果文本超过 max_length，则截断到指定的最大长度。\n",
    "                                   padding='max_length',#将文本填充到 max_length 指定的长度。\n",
    "                                   max_length=500,#指定最大长度为 500。\n",
    "                                   return_tensors='pt',#返回 PyTorch 张量格式。\n",
    "                                   return_length=True)#返回每个文本的实际长度。\n",
    "\n",
    "    \n",
    "\n",
    "    # 提取输入 id、attention mask 和 token type id\n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    token_type_ids = data['token_type_ids'].to(device)\n",
    "    labels = torch.LongTensor(labels).to(device)\n",
    "\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "#加载bert预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "pretrained = pretrained.to(device)\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "\n",
    "#定义下游任务模型，继承自 PyTorch 的 torch.nn.Module。该模型使用了预训练的 BERT 模型（pretrained），并添加了一个全连接层（fc）作为分类头。\n",
    "#定义模型类 Model\n",
    "class Model(torch.nn.Module):#这是一个继承自 torch.nn.Module 的自定义类，用于定义下游任务模型。\n",
    "    #是 PyTorch 中所有神经网络模型的基类，所有自定义模型都应该继承自它。\n",
    "    def __init__(self):\n",
    "        super().__init__()#super().__init__() 调用了父类（torch.nn.Module）的构造函数，确保模型正确初始化。\n",
    "        self.fc = torch.nn.Linear(768, 2)#定义了一个全连接层 fc，它的输入维度是 768，输出维度是 2。\n",
    "\n",
    "    \n",
    "    #2. 定义前向传播（forward）方法\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):#这是模型的前向传播方法，用于执行推理或训练过程中的计算。在前向传播中，数据将通过模型进行计算，并返回输出结果。\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,#是输入的文本（经过编码后的 token IDs）。\n",
    "                       attention_mask=attention_mask,#是用于指示哪些位置为填充的掩码（0 表示填充，1 表示有效 token）\n",
    "                       token_type_ids=token_type_ids)#是用于区分句子的标识符\n",
    "\n",
    "        # 提取 [CLS] token 进行分类\n",
    "        cls_embedding = out.last_hidden_state[:, 0]  # 形状: (batch_size, hidden_size)\n",
    "        \n",
    "        # 将输出移动到正确的设备上\n",
    "        cls_embedding = cls_embedding.to(input_ids.device)  # 确保输出在与输入相同的设备上\n",
    "        \n",
    "        out = self.fc(cls_embedding)\n",
    "\n",
    "        #应用 Softmax 激活函数\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8c7abbe-2ce6-4655-84ca-6057673401ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6908183097839355 0.5625 0.4625 0 0\n",
      "5 0.6885359883308411 0.625 0.56875 0.4625 0\n",
      "10 0.6132141351699829 0.8125 0.74375 0.56875 0\n",
      "15 0.6094036102294922 0.625 0.71875 0.74375 1\n",
      "20 0.6087853908538818 0.75 0.80625 0.74375 0\n",
      "25 0.6029424071311951 0.8125 0.775 0.80625 1\n",
      "30 0.566361129283905 0.875 0.775 0.80625 2\n",
      "35 0.573114275932312 0.8125 0.8625 0.80625 0\n",
      "40 0.5539938807487488 0.8125 0.80625 0.8625 1\n",
      "45 0.5200412273406982 0.9375 0.85625 0.8625 2\n",
      "50 0.5252194404602051 0.8125 0.85625 0.8625 3\n",
      "55 0.5761885643005371 0.8125 0.78125 0.8625 4\n",
      "60 0.5383076667785645 0.8125 0.875 0.8625 0\n",
      "65 0.44062837958335876 1.0 0.8625 0.875 1\n",
      "70 0.4612167477607727 0.9375 0.7875 0.875 2\n",
      "75 0.4838995933532715 0.875 0.875 0.875 3\n",
      "80 0.5190228819847107 0.8125 0.825 0.875 4\n",
      "85 0.5019806623458862 0.9375 0.88125 0.875 0\n",
      "90 0.6404908895492554 0.625 0.85 0.88125 1\n",
      "95 0.45633289217948914 0.875 0.83125 0.88125 2\n",
      "100 0.5419474840164185 0.875 0.83125 0.88125 3\n",
      "105 0.434012234210968 0.9375 0.8625 0.88125 4\n",
      "110 0.42096593976020813 1.0 0.89375 0.88125 0\n",
      "115 0.4862874150276184 0.875 0.85625 0.89375 1\n",
      "120 0.4421497583389282 0.9375 0.83125 0.89375 2\n",
      "125 0.4694159924983978 1.0 0.8375 0.89375 3\n",
      "130 0.5171829462051392 0.8125 0.89375 0.89375 4\n",
      "135 0.46843409538269043 0.875 0.85 0.89375 5\n",
      "140 0.5256807804107666 0.75 0.9 0.89375 0\n",
      "145 0.46079325675964355 0.875 0.8625 0.9 1\n",
      "150 0.43723997473716736 0.9375 0.8625 0.9 2\n",
      "155 0.5083191394805908 0.8125 0.88125 0.9 3\n",
      "160 0.5358211398124695 0.875 0.85 0.9 4\n",
      "165 0.4636123478412628 0.875 0.88125 0.9 5\n",
      "170 0.5966358780860901 0.75 0.86875 0.9 6\n",
      "175 0.5428869724273682 0.75 0.85625 0.9 7\n",
      "180 0.43279820680618286 0.9375 0.85625 0.9 8\n",
      "185 0.42775869369506836 0.9375 0.8625 0.9 9\n",
      "190 0.5069368481636047 0.8125 0.85625 0.9 10\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    #初始化正确预测和总样本数\n",
    "    correct = 0#初始化用于计数预测正确的样本数\n",
    "    total = 0#初始化用于计数所有样本的总数。\n",
    "\n",
    "    #创建测试数据加载器\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "    #开始遍历测试集\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):  \n",
    "\n",
    "        #遍历5次测试集进行模型测试\n",
    "        if i == 5:\n",
    "            break\n",
    "            \n",
    "        #这个上下文管理器用于禁用梯度计算，表示我们在测试阶段只需要执行前向传播，并不需要计算梯度（不需要反向传播），这能节省内存和计算资源。\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "        \n",
    "        #计算预测结果并累积正确率\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "        accuracy = correct / total\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#创建和使用模型实例转移到gpu上运行\n",
    "model = Model().to(device)\n",
    "\n",
    "#训练\n",
    "#这里我们使用了 AdamW 优化器，它是基于 Adam 优化器的变体，加入了权重衰减（weight decay）机制，通常用来训练带有预训练模型的任务（如 BERT）。\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "\n",
    "#指定了 交叉熵损失函数（CrossEntropyLoss），它是分类任务中常用的损失函数。对于多分类任务，交叉熵损失函数比较适合。\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 模型训练过程\n",
    "model.train()#切换模型到训练模式。这是一个 PyTorch 的方法，它启用了 Dropout 和 Batch Normalization 等训练时的特殊行为。训练时和推理时（model.eval()）的行为是不同的。\n",
    "after_large_test_accuracy = 0#初始化临时存放的评估准确率\n",
    "num=0#准确率评估次数\n",
    "\n",
    "#这是一个批量迭代过程，loader 是训练数据的 DataLoader。每次迭代中，从 loader 中获取一批数据，包括：\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "\n",
    "\n",
    "    #前向传播（计算模型输出）\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "    #计算损失并进行反向传播\n",
    "    loss = criterion(out, labels)#criterion(out, labels)：CrossEntropyLoss 会自动对 out 应用 softmax（将 logits 转化为概率），然后计算损失。\n",
    "    loss.backward()#loss.backward()：进行 反向传播，计算损失函数相对于模型参数的梯度。这个步骤会通过链式法则计算每个模型参数的梯度。\n",
    "    optimizer.step()#optimizer.step()：使用优化器根据计算得到的梯度来更新模型的参数。即通过梯度下降算法调整权重，使损失函数最小化。\n",
    "    optimizer.zero_grad()#optimizer.zero_grad()：在每次更新参数之前，清除之前计算的梯度。这是因为 PyTorch 中的梯度是累加的，因此每次梯度更新后都需要手动清空之前的梯度，否则它们会相加，影响下一次更新。\n",
    "   \n",
    "    if i % 5 == 0:#每经过 5 次迭代（即每 5 个 batch）打印一次训练信息，包括当前的损失和准确率。\n",
    "        out = out.argmax(dim=1)#out.argmax(dim=1)：argmax(dim=1) 会返回 out 中每个样本的最大值索引，这个索引对应的是模型的预测类别。dim=1 表示沿类别维度进行最大值索引操作。\n",
    "        \n",
    "        accuracy = (out == labels).sum().item() / len(labels)#accuracy = (out == labels).sum().item() / len(labels)：计算当前批次的 准确率。通过比较预测结果（out）与实际标签（labels），得出预测正确的样本数，然后除以批次的总样本数，得到准确率。\n",
    "\n",
    "        temp_test_accuracy = test()\n",
    "        front_large_test_accuracy = after_large_test_accuracy#保存准确率最大时的数据\n",
    "        #每一次训练模型后进行模型评估,采用提取终止策略,时刻监视测试集上的准确率，保存测试集上准确率最大时刻的模型，提高模型泛化性能        \n",
    "        if temp_test_accuracy>after_large_test_accuracy:\n",
    "            num = 0\n",
    "            after_large_test_accuracy = temp_test_accuracy \n",
    "            #torch.save(model.state_dict(), 'sentiment_model.pth')  \n",
    "            # 保存整个模型\n",
    "            torch.save(model, './model/sentiment_model_full.pth')\n",
    "        elif front_large_test_accuracy == after_large_test_accuracy:\n",
    "             num +=1\n",
    "             if num>10:\n",
    "               break  \n",
    "\n",
    "        #打印当前的迭代步数（i）、损失值（loss.item()）和准确率，当前时刻测试集准确率、测试集最大准确率持续次数\n",
    "        print(i, loss.item(), accuracy,temp_test_accuracy,front_large_test_accuracy,num)\n",
    "\n",
    "\n",
    "   #终止条件（训练最多进行 301 次迭代）\n",
    "    if i == 300:\n",
    "        break\n",
    "        #if i == 300:：当迭代次数达到 300 时，手动 终止训练。这意味着最多进行 301 个批次的训练（从 0 开始）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c8a534-837f-4dcd-b46f-3dd09e73c1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11952\\AppData\\Local\\Temp\\ipykernel_22144\\3974306987.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./model/sentiment_model_full.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#基于训练好的模型对评论进行测试\n",
    "# 加载整个模型\n",
    "model = torch.load('./model/sentiment_model_full.pth')\n",
    "model.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04901237-5393-4c2c-9060-cbfc2eb16735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-chinese')\n",
    "def preprocess_text(text, max_length=128):\n",
    "    # 对文本进行编码，转换为 BERT 所需的输入格式\n",
    "    encoding = tokenizer(text, \n",
    "                         truncation=True, \n",
    "                         padding='max_length', \n",
    "                         max_length=max_length, \n",
    "                         return_tensors='pt')\n",
    "    return encoding\n",
    "\n",
    "# 示例文本\n",
    "text = \"这部电影真的很棒！\"\n",
    "\n",
    "# 预处理文本\n",
    "encoding = preprocess_text(text)\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae84d6cc-eecc-425d-8a69-7b2ea5a7a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情感分析结果：正面\n"
     ]
    }
   ],
   "source": [
    "# 进行情感预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    token_type_ids=None)  # 如果使用句子对任务，可以设置 token_type_ids\n",
    "\n",
    "# 获取预测的类别，假设是二分类任务，0表示负面，1表示正面\n",
    "pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# 打印预测结果\n",
    "if pred.item() == 1:\n",
    "    print(\"情感分析结果：正面\")\n",
    "else:\n",
    "    print(\"情感分析结果：负面\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "230790a0-a7c6-447c-ba94-abf355b5dd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: 这是一座晚上最出片的宝藏寺庙。大佛古寺位于广州北京路商圈中心，夜晚的大佛寺尤为亮眼，琉璃金光笼罩着整个寺庙，金碧辉煌，气势辉煌，在灯光的映照下，可谓现实版的千与千寻非常适合拍照，随便一个角落都能拍出美美的照片。庙内的每尊佛像都十分精美壮观。哈喽。！ -> 情感分析结果: 正面\n",
      "文本: 这部电影非常糟糕。 -> 情感分析结果: 负面\n"
     ]
    }
   ],
   "source": [
    "# 假设有多个文本进行批量预测\n",
    "texts = [\"这是一座晚上最出片的宝藏寺庙。大佛古寺位于广州北京路商圈中心，夜晚的大佛寺尤为亮眼，琉璃金光笼罩着整个寺庙，金碧辉煌，气势辉煌，在灯光的映照下，可谓现实版的千与千寻非常适合拍照，随便一个角落都能拍出美美的照片。庙内的每尊佛像都十分精美壮观。哈喽。！\", \"这部电影非常糟糕。\"]\n",
    "\n",
    "# 预处理所有文本\n",
    "encodings = [preprocess_text(text) for text in texts]\n",
    "input_ids_batch = torch.cat([encoding['input_ids'] for encoding in encodings], dim=0).to(device)\n",
    "attention_mask_batch = torch.cat([encoding['attention_mask'] for encoding in encodings], dim=0).to(device)\n",
    "\n",
    "# 进行情感预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids_batch, \n",
    "                    attention_mask=attention_mask_batch, \n",
    "                    token_type_ids=None)\n",
    "\n",
    "# 获取每个文本的预测结果\n",
    "predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# 打印预测结果\n",
    "for text, pred in zip(texts, predictions):\n",
    "    sentiment = \"正面\" if pred.item() == 1 else \"负面\"\n",
    "    print(f\"文本: {text} -> 情感分析结果: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca91c1-e2be-4a1a-91a4-63aada126837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mypytorch1)",
   "language": "python",
   "name": "mypytorch1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
